

The Grow and Shrink algorithms for 
vectors both need to test whether a 
vector is superfluous, so we need a 
criterion for testing that. 
So here it is. 
For any set of vectors, and any vector v 
in that set, If v can be written as a 
linear combination of the other vectors 
in that set, then it's not needed. 
Span of the set without that vector 
equals span of the set with that vector. 
And here's the proof. 
Let S be the set of vectors v1 through 
vn. 
And suppose v n can be written as a 
linear combination of the other vectors. 
We need to show that every vector in the 
span of S is also in span of the set 
obtained from S by removing v n. 
Every vector in span of S can be written 
as a linear combination of the vectors V1 
through Vn. 
Substituting for Vn in this expression we 
obtain v equals this linear combination, 
where notice that Vn does not appear. 
We can rewrite that so that it's obvious 
it's just a linear combination in the 
vector v1 through vn minus 1. 
And these are the coefficients. 
So this shows that the vector, v, 
arbitrary vector in Span of S. 
Can be written as a linear combination of 
the vectors in S minus vn. 
And is therefore in the Span of that set. 
That completes the proof. 
We say that vectors v1 through vn are 
linearly dependent. 
If the zero vector can be written as a 
nontrivial linear combination of those 
vectors. 
Nontrivial means that at least one of 
those coefficients is non zero. 
In that case, we'd refer to this linear 
combination as a linear dependency in the 
vectors v one through vn. 
On the other hand, if the only linear 
combination that yields zero is the one 
where the coefficients are all zero, we 
say the vectors v1 through vn are 
linearly independent. 
Now here's an example. 
These vectors are linearly dependent. 
And here's a way of writing the 0 vector 
as a nontrivial linear combination of the 
vectors. 
That shows that they're linearly 
dependent and this is a linear dependency 
in those vectors. 
Here's another example. 
These vectors are linearly independent. 
How do we know? 
In this case, the structure of the 
vectors makes it easy to tell. 
Consider any linear combination of these 
vectors. 
Summing it up we get the vector alpha 1, 
2 alpha 2 and 4 alpha 3. 
So, if this linear combination equals the 
zero vector. 
It must be, that alpha 1 equals 0, 2 
alpha 2 equals 0, therefore alpha 2 
equals 0 and 4 alpha 3 equals 0. 
Therefore, alpha 3 equals 0. 
So, that shows, that in that case, the 
linear combination is trivial. 
So, we've show, that the only linear 
combination of these vectors, that equals 
the 0 vector. 
Is the trivial linear combination, and 
therefore these vectors are linearly 
independent. 
How can we tell if vectors v1 through vn 
are linearly dependent? 
We're not yet in a position to answer 
that question. 
But we can relate it to other questions 
we've already asked Recall the 
definitions. 
Vectors v one through v n are linearly 
dependent if the zero vector can be 
written as a nontrivial linear 
combination of v one through v n. 
Now, by the linear combinations 
definition of matrix vector 
multiplication, this linear combination 
can be written as a matrix vector product 
where the columns of the matrix are the 
vectors v1 through vn. 
And the vector consists of the 
coefficients in linear combination. 
It follows that vectors v1 though vn are 
linearly dependent if and only if there 
is a nonzero vector Such that multiplying 
that vector by this matrix yields the 
zero vector. 
Therefore, the vectors v1 through v n are 
linearly dependent if and only if the 
null space of this matrix is nontrivial. 
This shows that the question, how can we 
tell if v1 through v n are linearly 
dependent Is the same as the question how 
can we tell if the null space of a matrix 
is trivial? 
So we see that, once we know how to 
answer one of these questions, we know 
how to answer the other. 
Here's another example. 
Consider a homogeneous linear system, 
that is, a linear system in which all the 
right hand side values are 0. 
Now, take these vectors, a1 through am, 
and make them the rows of a matrix. 
Then, the set of solutions to this 
homogeneous linear system (no period) Is 
the same as the null space of this 
matrix. 
That tells us that this question, how can 
we tell if a null space of a matrix is 
trivial, is the same as the question, how 
can we tell if the solution set of a 
homogeneous linear system is trivial. 
So we see that these three questions are 
really different forms of the same 
underlying question. 
Here's yet one more example. 
Recall that if u1 is a solution to a 
linear system, then the set of all 
solutions to this linear system Is 
exactly the set consisting of all vectors 
u 1 plus v, where v belongs to the set of 
solutions to the corresponding 
homogeneous linear system. 
Thus the questions we've asked are also 
the same as the question, how can we tell 
if a solution u 1 to a linear system Is 
the only solution to that linear system. 
So we've seen that all these questions 
are really disguise versions of the same 
question. 
Answering these questions requires an 
algorithm. 
So, this is a fundamental computational 
problem. 
Testing linear dependents, given a list 
of vectors V1 through Vn Determine 
whether they are dependent or 
independent. 
And we'll see two algorithms for this 
later on. 
Now, in minimum spanning forest, a zero 
vector can be obtained by taking the 
vectors that correspond to edges that 
form a cycle. 
So in such a sum, for each node, or each 
each label, there are exactly two vectors 
that have a one in that position so 
cancellation gives you a zero. 
So, for example, take the vectors 
corresponding to Main Wriston, Main 
Keeney and Keeney Wriston. 
In each position, there are an even 
number of ones. 
So, by summing these vectors we get the 
zero vector. 
Now, if you have a set of edges. 
That include a cycle as a subset. 
Then there is a linear combination of the 
corresponding vectors that equals zero. 
So, take the vectors corresponding to the 
dark edges in this diagram. 
They are linearly dependent, because A 
subset of those edges forms a cycle. 
So, we can write, the zero vector as a 
linear combination of the vectors 
corresponding to these dark edges. 
We assign coefficients one to each of the 
first three, and zero to the last. 
And summing them up we get 0 for each 
position. 
What we see if a sub set of S forms a 
cycle, then S is linearly deep. 
On the other hand if a set of edges 
Contains no cycle, that is, if it's a 
forest, then the set of edges corresponds 
to a linearly independent set of vectors. 
In this case, the dark edges form a 
forest, there are no cycles, so the corresponding 
set of vectors is linearly independent. 
In general, a subset of linearly 
independent set is a linearly independent 
set. 
In the case of graphs, if the set of 
edges forms no cycle then taking away 
edges from that set You still can't form 
a cycle. 
So let's go through the proof of this 
lemma. 
Let s and t be two subsets of vectors and 
suppose s is a subset of t. 
We want to prove that if t is linearly 
independent, then s is linearly 
independent. 
Now that's equivalent to the 
contrapositive. 
If it's less, if S is linearly dependent, 
then T is linearly dependent, and we'll 
prove it in that form. 
The idea is very simple. 
If you can write 0 as a linear 
combination of some vectors, then if you 
throw in some more vectors, you can 
certainly write 0 as a non-trivial linear 
combination of all the vectors. 
You can just assign 0 to the extra 
vectors. 
So, let's go through that formally. 
So we'll write T as this set of vectors, 
where S is the subset consisting of the 
first N. 
Now, suppose that S is linearly 
dependent. 
That is, there are coefficients, not all 
0 
with which you can write a linear 
combination that equals the 0 vector. 
Well, we throw in the additional vectors 
t1 through tk. 
We can, the same linear combination, 
together with 0 coefficients on the 
additional vectors. 
Still gives us the zero vector, and it's 
still a nontrivial linear combination, 
because at least one of the alpha 1 
through alpha n is nonzero. 
What that shows that the set of vectors 
in big T is linearly dependent. 
And that proves the lemma. 
Here's another lemma that's going to be 
useful in reasoning about linear 
dependence. 
Let v on through vn be vectors. 
One of the these vectors, vi, is in the 
span of the other vectors if and only if 
the zero vector can be written as a 
linear combination of all these vectors. 
Where the coefficient of vi is non-zero. 
In graphs, for example, the 
Linear-Dependence Lemma corresponds to 
the fact that an edge is in the span of 
some other edges if and only if there's a 
cycle consisting of some of those other 
edges together with the edge. 
Let's go through the proof. 
The proof has two directions like many, 
only if statements. 
Let's do the first direction. 
Suppose Vi is in the span of the other 
vectors, that is you can write the I as 
an linear combination of all the other 
vectors. 
Now we just move the i to the other side, 
and we get this equation. 
Well here, we've written the zero vector 
as a linear combination of all the 
vectors, including vi. 
And vi's coefficient is not zero. 
That proves one direction. 
Let's prove the other directions. 
Suppose that there are coefficients, such 
that the 0 vector can be written as a 
linear combination of the vectors, where 
the coeffiecient of V, I, is non-zero. 
Divide all this by that coefficient. 
And you get this equation. 
And now we can move everything to the 
other side except V1, and we've managed 
to express V1 as a linear combination of 
the other vectors. 
That proves the other direction. 
We can restate this in the 
contrapositive. 
This is an equivalent statement. 
Vi is not in the span of the other 
vectors, if and only if for every linear 
combination equaling zero, the 
coefficient of vi must be zero. 
We'll use this to argue about the grow 
algorithm. 
We'll show that the vectors obtained by 
the grow algorithm are linearly 
independent. 
In graphs that means that when you run 
the grow algorithm 
you'll end up with a forest, a set of 
edges that contain no cycle. 
So now let's prove this corollary. 
For n equal 1, 2 and so on, let vn be the 
vector added. 
In the nth iteration of the grow 
algorithm. 
And we'll show by induction that the 
vectors v one through v n are all 
linearly independent. 
Here's the basis of the induction. 
For n equals zero, there are no vectors 
so the claim that they're linearly 
independent is trivially true. 
Assume the claim holds for N equals K 
minus 1, the induction step consists of 
then showing that it's true for n equals 
K. 
The vector, Vk, added to S in the Kth 
iteration is not in the span of the 
previously added vectors, V1 through Vk 
minus 1. 
Therefore by the linear dependence lemma, 
if you write zero as a linear combination 
of the vectors v 1 through v k. 
It must be that the coefficient of v k 
equals zero. 
You can rewrite the equation this way 
without vk. 
But By the inductive hypothesis, the 
claim for n equals k minus 1, the vectors 
v1 through vk minus 1 are linearly 
independent. 
So that, that implies that all these 
coefficients must be 0. 
And that shows that the linear 
combination is trivial. 
And that is that the vectors v1 through 
vk are linearly independent. 
And that proves the claim for n equals k. 
By induction, it proves the whole 
corollary. 
Let's look at the Shrink-Algorithm. 
When the Shrink-Algorithm stops 
the set of vectors in S is literally 
independent. 
In graphs, that means the Shrink 
algorithm produces a forest. 
We're going to use the Superfluous-Vector 
Lemma. 
Remember, that for any set S of vectors 
and any vector v in S, if v can be 
written. 
As a linear combination of the vectors, 
the other vectors, then v is superfluous. 
The span of S without v is the same as 
the span of S with v. 
And now we've proved that the Shrink 
algorithm eventually produces a linearly 
independent set of vectors. 
Let v1 through vn be the set of vectors 
obtained by the Shrink algorithm. 
And assume for a contradiction that these 
vectors are linearly dependent. 
That means that the zero vector can be 
written as a nontrivial linear 
combination of these vectors, that is, a 
linear combination in which at least one 
of those coefficients is non-zero. 
So let alpha I be one of those non-zero 
coefficients. 
By the linear dependency Lemma then, vi, 
 the corresponding vector, can be 
written as a linear combination of the 
other vectors. 
By the superfluous vector Lemma we don't 
need vi. 
The span of the set without vi is the 
span with vi. 
But that means that the shrink algorithm 
would have removed vi, because it wasn't 
necessary. 
That completes the proof. 

